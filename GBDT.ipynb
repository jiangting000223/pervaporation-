{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c0ce24b8-1f6b-47aa-8a98-9535b77b4dc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数据加载成功 (UTF-8)\n",
      "原始数据总量: 816\n",
      "用于调优和CV的训练集 (80%): 652\n",
      "独立测试集 (20%): 164\n",
      "\n",
      "==================================================\n",
      ">>> Stage 1: 开始 HGBR 贝叶斯优化 (BayesSearchCV)...\n",
      "==================================================\n",
      "贝叶斯优化耗时: 51.07 秒\n",
      "\n",
      ">>> 最佳参数:\n",
      "  regressor__l2_regularization: 1.0\n",
      "  regressor__learning_rate: 0.16909958792303756\n",
      "  regressor__max_depth: 17\n",
      "  regressor__max_iter: 500\n",
      "  regressor__min_samples_leaf: 24\n",
      "\n",
      "==================================================\n",
      ">>> Stage 2: 使用最佳参数进行十折交叉验证 (10-Fold CV)...\n",
      "==================================================\n",
      "Fold 1/10 完成 | Val R2: 0.7730\n",
      "Fold 2/10 完成 | Val R2: 0.7896\n",
      "Fold 3/10 完成 | Val R2: 0.8156\n",
      "Fold 4/10 完成 | Val R2: 0.8791\n",
      "Fold 5/10 完成 | Val R2: 0.8759\n",
      "Fold 6/10 完成 | Val R2: 0.9384\n",
      "Fold 7/10 完成 | Val R2: 0.7661\n",
      "Fold 8/10 完成 | Val R2: 0.7233\n",
      "Fold 9/10 完成 | Val R2: 0.8857\n",
      "Fold 10/10 完成 | Val R2: 0.7949\n",
      "\n",
      ">>> 十折交叉验证详细结果 (Per-Fold Results):\n",
      "       Fold  Train R2  Train RMSE  Train MAE  Val R2  Val RMSE  Val MAE\n",
      "0         1    0.9973      0.0381     0.0187  0.7730    0.3643   0.2108\n",
      "1         2    0.9963      0.0458     0.0219  0.7896    0.2505   0.1520\n",
      "2         3    0.9966      0.0430     0.0208  0.8156    0.3069   0.1905\n",
      "3         4    0.9959      0.0483     0.0227  0.8791    0.2183   0.1509\n",
      "4         5    0.9978      0.0352     0.0191  0.8759    0.2220   0.1450\n",
      "5         6    0.9954      0.0467     0.0219  0.9384    0.2723   0.1813\n",
      "6         7    0.9960      0.0474     0.0213  0.7661    0.3297   0.1983\n",
      "7         8    0.9960      0.0476     0.0211  0.7233    0.3013   0.1749\n",
      "8         9    0.9961      0.0456     0.0206  0.8857    0.2639   0.1729\n",
      "9        10    0.9954      0.0497     0.0226  0.7949    0.3553   0.2011\n",
      "10  Average    0.9963      0.0447     0.0211  0.8242    0.2884   0.1778\n",
      "\n",
      "==================================================\n",
      ">>> Stage 3: 独立测试集 (Hold-out Test Set) 最终评估...\n",
      "==================================================\n",
      "\n",
      ">>> 模型性能最终汇总 (HistGradientBoosting + BayesOpt):\n",
      "  Metric  CV Training (Avg)  CV Validation (Avg)  Test Set (Final)\n",
      "0    MAE             0.0211               0.1778            0.1592\n",
      "1   RMSE             0.0447               0.2884            0.2282\n",
      "2     R2             0.9963               0.8242            0.8823\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# 引入贝叶斯优化库\n",
    "from skopt import BayesSearchCV\n",
    "from skopt.space import Real, Categorical, Integer\n",
    "\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.base import clone\n",
    "\n",
    "# ==========================================\n",
    "# 1. 数据加载与分割 (80:20)\n",
    "# ==========================================\n",
    "file_path = 'C:/Users/tinid/polymer/major revision/标准化数据_无独热_Log变换12.18.csv'\n",
    "\n",
    "try:\n",
    "    data = pd.read_csv(file_path)\n",
    "    print(\"数据加载成功 (UTF-8)\")\n",
    "except UnicodeDecodeError:\n",
    "    data = pd.read_csv(file_path, encoding='gbk')\n",
    "    print(\"数据加载成功 (GBK)\")\n",
    "\n",
    "excluded_columns = ['log_Separation factor', 'polymer', 'DOI', 'Flux']\n",
    "X = data.drop(columns=excluded_columns, errors='ignore')\n",
    "y = data['log_Separation factor']\n",
    "\n",
    "# 划分出 20% 的独立测试集 (Test Set)\n",
    "# 剩下的 80% (X_train_full) 用于调参和交叉验证\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"原始数据总量: {len(X)}\")\n",
    "print(f\"用于调优和CV的训练集 (80%): {len(X_train_full)}\")\n",
    "print(f\"独立测试集 (20%): {len(X_test)}\")\n",
    "\n",
    "# ==========================================\n",
    "# 2. 构建预处理与模型流水线\n",
    "# ==========================================\n",
    "categorical_features = ['Permeation type']\n",
    "numerical_features = [col for col in X.columns if col not in categorical_features]\n",
    "\n",
    "# 定义预处理器\n",
    "# 注意：HistGradientBoostingRegressor 对数值缩放不敏感，且你已经标准化过，\n",
    "# 所以数值列使用 'passthrough' (不做处理)\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', 'passthrough', numerical_features),\n",
    "        ('cat', OneHotEncoder(sparse_output=False, handle_unknown='ignore'), categorical_features)\n",
    "    ])\n",
    "\n",
    "# 定义 Pipeline: 预处理 -> HGBR\n",
    "pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('regressor', HistGradientBoostingRegressor(random_state=42))\n",
    "])\n",
    "\n",
    "# ==========================================\n",
    "# 3. 第一阶段：贝叶斯超参数优化\n",
    "# ==========================================\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\">>> Stage 1: 开始 HGBR 贝叶斯优化 (BayesSearchCV)...\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# 定义搜索空间\n",
    "# HGBR 的关键参数与普通 GBDT 略有不同\n",
    "search_spaces = {\n",
    "    'regressor__learning_rate': Real(0.01, 0.3, prior='log-uniform'), # 学习率\n",
    "    'regressor__max_iter': Integer(50, 500),         # 树的数量 (迭代次数)\n",
    "    'regressor__max_depth': Integer(3, 20),          # 树深 (HGBR通常较深)\n",
    "    'regressor__min_samples_leaf': Integer(10, 50),  # 叶节点最小样本数\n",
    "    'regressor__l2_regularization': Real(0.0, 1.0)   # L2 正则化\n",
    "}\n",
    "\n",
    "# 初始化贝叶斯搜索\n",
    "opt = BayesSearchCV(\n",
    "    pipeline,\n",
    "    search_spaces,\n",
    "    n_iter=30,      # 迭代 30 次\n",
    "    cv=5,           # 调优阶段内部 5 折\n",
    "    n_jobs=-1,      # 并行计算\n",
    "    random_state=42,\n",
    "    scoring='neg_mean_absolute_error',\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "start_time = time.time()\n",
    "opt.fit(X_train_full, y_train_full)\n",
    "end_time = time.time()\n",
    "\n",
    "best_estimator = opt.best_estimator_ # 包含预处理的完整 Pipeline\n",
    "best_params = opt.best_params_\n",
    "\n",
    "print(f\"贝叶斯优化耗时: {end_time - start_time:.2f} 秒\")\n",
    "print(\"\\n>>> 最佳参数:\")\n",
    "for param, value in best_params.items():\n",
    "    print(f\"  {param}: {value}\")\n",
    "\n",
    "# ==========================================\n",
    "# 4. 第二阶段：十折交叉验证 (使用最佳参数)\n",
    "# ==========================================\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\">>> Stage 2: 使用最佳参数进行十折交叉验证 (10-Fold CV)...\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "kf = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "fold_results = []\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(X_train_full)):\n",
    "    # 4.1 切分数据\n",
    "    X_train_fold = X_train_full.iloc[train_idx]\n",
    "    y_train_fold = y_train_full.iloc[train_idx]\n",
    "    X_val_fold = X_train_full.iloc[val_idx]\n",
    "    y_val_fold = y_train_full.iloc[val_idx]\n",
    "    \n",
    "    # 4.2 克隆最佳模型\n",
    "    model = clone(best_estimator)\n",
    "    \n",
    "    # 4.3 训练 (Pipeline 自动处理 OneHot)\n",
    "    model.fit(X_train_fold, y_train_fold)\n",
    "    \n",
    "    # 4.4 预测\n",
    "    y_train_pred = model.predict(X_train_fold)\n",
    "    y_val_pred = model.predict(X_val_fold)\n",
    "    \n",
    "    # 4.5 记录指标\n",
    "    metrics = {\n",
    "        \"Fold\": fold + 1,\n",
    "        \"Train R2\": r2_score(y_train_fold, y_train_pred),\n",
    "        \"Train RMSE\": np.sqrt(mean_squared_error(y_train_fold, y_train_pred)),\n",
    "        \"Train MAE\": mean_absolute_error(y_train_fold, y_train_pred),\n",
    "        \"Val R2\": r2_score(y_val_fold, y_val_pred),\n",
    "        \"Val RMSE\": np.sqrt(mean_squared_error(y_val_fold, y_val_pred)),\n",
    "        \"Val MAE\": mean_absolute_error(y_val_fold, y_val_pred)\n",
    "    }\n",
    "    fold_results.append(metrics)\n",
    "    print(f\"Fold {fold+1}/10 完成 | Val R2: {metrics['Val R2']:.4f}\")\n",
    "\n",
    "# 输出详细表格\n",
    "df_cv_results = pd.DataFrame(fold_results)\n",
    "avg_row = df_cv_results.mean(numeric_only=True).to_frame().T\n",
    "avg_row[\"Fold\"] = \"Average\"\n",
    "df_final_cv = pd.concat([df_cv_results, avg_row], ignore_index=True)\n",
    "\n",
    "print(\"\\n>>> 十折交叉验证详细结果 (Per-Fold Results):\")\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', 1000)\n",
    "print(df_final_cv.round(4))\n",
    "\n",
    "# ==========================================\n",
    "# 5. 第三阶段：独立测试集最终评估\n",
    "# ==========================================\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\">>> Stage 3: 独立测试集 (Hold-out Test Set) 最终评估...\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# 使用 80% 的全量数据重新训练最佳模型\n",
    "best_estimator.fit(X_train_full, y_train_full)\n",
    "\n",
    "# 在 20% 独立测试集上预测\n",
    "y_test_pred = best_estimator.predict(X_test)\n",
    "\n",
    "final_summary = {\n",
    "    \"Metric\": [\"MAE\", \"RMSE\", \"R2\"],\n",
    "    \"CV Training (Avg)\": [\n",
    "        df_final_cv.iloc[-1][\"Train MAE\"],\n",
    "        df_final_cv.iloc[-1][\"Train RMSE\"],\n",
    "        df_final_cv.iloc[-1][\"Train R2\"]\n",
    "    ],\n",
    "    \"CV Validation (Avg)\": [\n",
    "        df_final_cv.iloc[-1][\"Val MAE\"],\n",
    "        df_final_cv.iloc[-1][\"Val RMSE\"],\n",
    "        df_final_cv.iloc[-1][\"Val R2\"]\n",
    "    ],\n",
    "    \"Test Set (Final)\": [\n",
    "        mean_absolute_error(y_test, y_test_pred),\n",
    "        np.sqrt(mean_squared_error(y_test, y_test_pred)),\n",
    "        r2_score(y_test, y_test_pred)\n",
    "    ]\n",
    "}\n",
    "\n",
    "print(\"\\n>>> 模型性能最终汇总 (HistGradientBoosting + BayesOpt):\")\n",
    "print(pd.DataFrame(final_summary).round(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d55d6a3b-c130-437b-b8f2-a90795f60784",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原始数据总量: 791\n",
      "用于调优和CV的训练集 (80%): 632\n",
      "独立测试集 (20%): 159\n",
      "\n",
      "==================================================\n",
      ">>> Stage 1: 开始 HGBR 贝叶斯优化 (BayesSearchCV)...\n",
      "==================================================\n",
      "贝叶斯优化耗时: 51.73 秒\n",
      "\n",
      ">>> 最佳参数:\n",
      "  regressor__l2_regularization: 1.0\n",
      "  regressor__learning_rate: 0.06146090472460746\n",
      "  regressor__max_depth: 20\n",
      "  regressor__max_iter: 433\n",
      "  regressor__min_samples_leaf: 10\n",
      "\n",
      "==================================================\n",
      ">>> Stage 2: 使用最佳参数进行十折交叉验证 (10-Fold CV)...\n",
      "==================================================\n",
      "Fold 1/10 完成 | Val R2: 0.4936\n",
      "Fold 2/10 完成 | Val R2: 0.8444\n",
      "Fold 3/10 完成 | Val R2: 0.5087\n",
      "Fold 4/10 完成 | Val R2: 0.6843\n",
      "Fold 5/10 完成 | Val R2: 0.6249\n",
      "Fold 6/10 完成 | Val R2: 0.7625\n",
      "Fold 7/10 完成 | Val R2: 0.7762\n",
      "Fold 8/10 完成 | Val R2: 0.6945\n",
      "Fold 9/10 完成 | Val R2: 0.8303\n",
      "Fold 10/10 完成 | Val R2: 0.7137\n",
      "\n",
      ">>> 十折交叉验证详细结果 (Per-Fold Results):\n",
      "       Fold  Train R2  Train RMSE  Train MAE  Val R2  Val RMSE  Val MAE\n",
      "0         1    0.9935      0.0503     0.0283  0.4936    0.4236   0.2556\n",
      "1         2    0.9948      0.0452     0.0264  0.8444    0.2274   0.1674\n",
      "2         3    0.9946      0.0463     0.0255  0.5087    0.3801   0.2162\n",
      "3         4    0.9932      0.0499     0.0279  0.6843    0.4234   0.2723\n",
      "4         5    0.9927      0.0528     0.0280  0.6249    0.3920   0.2680\n",
      "5         6    0.9947      0.0455     0.0272  0.7625    0.2983   0.1924\n",
      "6         7    0.9945      0.0467     0.0273  0.7762    0.2661   0.1787\n",
      "7         8    0.9929      0.0528     0.0292  0.6945    0.3112   0.1960\n",
      "8         9    0.9905      0.0592     0.0300  0.8303    0.2968   0.2163\n",
      "9        10    0.9954      0.0430     0.0250  0.7137    0.2872   0.1931\n",
      "10  Average    0.9937      0.0492     0.0275  0.6933    0.3306   0.2156\n",
      "\n",
      "==================================================\n",
      ">>> Stage 3: 独立测试集 (Hold-out Test Set) 最终评估...\n",
      "==================================================\n",
      "\n",
      ">>> 模型性能最终汇总 (HistGradientBoosting + BayesOpt):\n",
      "  Metric  CV Training (Avg)  CV Validation (Avg)  Test Set (Final)\n",
      "0    MAE             0.0275               0.2156            0.2082\n",
      "1   RMSE             0.0492               0.3306            0.3033\n",
      "2     R2             0.9937               0.6933            0.7636\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# 引入贝叶斯优化库\n",
    "from skopt import BayesSearchCV\n",
    "from skopt.space import Real, Categorical, Integer\n",
    "\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.base import clone\n",
    "\n",
    "# ==========================================\n",
    "# 1. 数据加载与分割 (80:20)\n",
    "# ==========================================\n",
    "file_path = 'C:/Users/tinid/polymer/major revision/通量标准化数据_无独热_Log变换12.18.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Define excluded columns (unchanged)\n",
    "excluded_columns = ['log_Flux', 'polymer', 'DOI', 'Separation factor']\n",
    "X = data.drop(columns=excluded_columns, errors='ignore')\n",
    "y = data['log_Flux']\n",
    "\n",
    "# 划分出 20% 的独立测试集 (Test Set)\n",
    "# 剩下的 80% (X_train_full) 用于调参和交叉验证\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"原始数据总量: {len(X)}\")\n",
    "print(f\"用于调优和CV的训练集 (80%): {len(X_train_full)}\")\n",
    "print(f\"独立测试集 (20%): {len(X_test)}\")\n",
    "\n",
    "# ==========================================\n",
    "# 2. 构建预处理与模型流水线\n",
    "# ==========================================\n",
    "categorical_features = ['Permeation type']\n",
    "numerical_features = [col for col in X.columns if col not in categorical_features]\n",
    "\n",
    "# 定义预处理器\n",
    "# 注意：HistGradientBoostingRegressor 对数值缩放不敏感，且你已经标准化过，\n",
    "# 所以数值列使用 'passthrough' (不做处理)\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', 'passthrough', numerical_features),\n",
    "        ('cat', OneHotEncoder(sparse_output=False, handle_unknown='ignore'), categorical_features)\n",
    "    ])\n",
    "\n",
    "# 定义 Pipeline: 预处理 -> HGBR\n",
    "pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('regressor', HistGradientBoostingRegressor(random_state=42))\n",
    "])\n",
    "\n",
    "# ==========================================\n",
    "# 3. 第一阶段：贝叶斯超参数优化\n",
    "# ==========================================\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\">>> Stage 1: 开始 HGBR 贝叶斯优化 (BayesSearchCV)...\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# 定义搜索空间\n",
    "# HGBR 的关键参数与普通 GBDT 略有不同\n",
    "search_spaces = {\n",
    "    'regressor__learning_rate': Real(0.01, 0.3, prior='log-uniform'), # 学习率\n",
    "    'regressor__max_iter': Integer(50, 500),         # 树的数量 (迭代次数)\n",
    "    'regressor__max_depth': Integer(3, 20),          # 树深 (HGBR通常较深)\n",
    "    'regressor__min_samples_leaf': Integer(10, 50),  # 叶节点最小样本数\n",
    "    'regressor__l2_regularization': Real(0.0, 1.0)   # L2 正则化\n",
    "}\n",
    "\n",
    "# 初始化贝叶斯搜索\n",
    "opt = BayesSearchCV(\n",
    "    pipeline,\n",
    "    search_spaces,\n",
    "    n_iter=30,      # 迭代 30 次\n",
    "    cv=5,           # 调优阶段内部 5 折\n",
    "    n_jobs=-1,      # 并行计算\n",
    "    random_state=42,\n",
    "    scoring='neg_mean_absolute_error',\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "start_time = time.time()\n",
    "opt.fit(X_train_full, y_train_full)\n",
    "end_time = time.time()\n",
    "\n",
    "best_estimator = opt.best_estimator_ # 包含预处理的完整 Pipeline\n",
    "best_params = opt.best_params_\n",
    "\n",
    "print(f\"贝叶斯优化耗时: {end_time - start_time:.2f} 秒\")\n",
    "print(\"\\n>>> 最佳参数:\")\n",
    "for param, value in best_params.items():\n",
    "    print(f\"  {param}: {value}\")\n",
    "\n",
    "# ==========================================\n",
    "# 4. 第二阶段：十折交叉验证 (使用最佳参数)\n",
    "# ==========================================\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\">>> Stage 2: 使用最佳参数进行十折交叉验证 (10-Fold CV)...\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "kf = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "fold_results = []\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(X_train_full)):\n",
    "    # 4.1 切分数据\n",
    "    X_train_fold = X_train_full.iloc[train_idx]\n",
    "    y_train_fold = y_train_full.iloc[train_idx]\n",
    "    X_val_fold = X_train_full.iloc[val_idx]\n",
    "    y_val_fold = y_train_full.iloc[val_idx]\n",
    "    \n",
    "    # 4.2 克隆最佳模型\n",
    "    model = clone(best_estimator)\n",
    "    \n",
    "    # 4.3 训练 (Pipeline 自动处理 OneHot)\n",
    "    model.fit(X_train_fold, y_train_fold)\n",
    "    \n",
    "    # 4.4 预测\n",
    "    y_train_pred = model.predict(X_train_fold)\n",
    "    y_val_pred = model.predict(X_val_fold)\n",
    "    \n",
    "    # 4.5 记录指标\n",
    "    metrics = {\n",
    "        \"Fold\": fold + 1,\n",
    "        \"Train R2\": r2_score(y_train_fold, y_train_pred),\n",
    "        \"Train RMSE\": np.sqrt(mean_squared_error(y_train_fold, y_train_pred)),\n",
    "        \"Train MAE\": mean_absolute_error(y_train_fold, y_train_pred),\n",
    "        \"Val R2\": r2_score(y_val_fold, y_val_pred),\n",
    "        \"Val RMSE\": np.sqrt(mean_squared_error(y_val_fold, y_val_pred)),\n",
    "        \"Val MAE\": mean_absolute_error(y_val_fold, y_val_pred)\n",
    "    }\n",
    "    fold_results.append(metrics)\n",
    "    print(f\"Fold {fold+1}/10 完成 | Val R2: {metrics['Val R2']:.4f}\")\n",
    "\n",
    "# 输出详细表格\n",
    "df_cv_results = pd.DataFrame(fold_results)\n",
    "avg_row = df_cv_results.mean(numeric_only=True).to_frame().T\n",
    "avg_row[\"Fold\"] = \"Average\"\n",
    "df_final_cv = pd.concat([df_cv_results, avg_row], ignore_index=True)\n",
    "\n",
    "print(\"\\n>>> 十折交叉验证详细结果 (Per-Fold Results):\")\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', 1000)\n",
    "print(df_final_cv.round(4))\n",
    "\n",
    "# ==========================================\n",
    "# 5. 第三阶段：独立测试集最终评估\n",
    "# ==========================================\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\">>> Stage 3: 独立测试集 (Hold-out Test Set) 最终评估...\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# 使用 80% 的全量数据重新训练最佳模型\n",
    "best_estimator.fit(X_train_full, y_train_full)\n",
    "\n",
    "# 在 20% 独立测试集上预测\n",
    "y_test_pred = best_estimator.predict(X_test)\n",
    "\n",
    "final_summary = {\n",
    "    \"Metric\": [\"MAE\", \"RMSE\", \"R2\"],\n",
    "    \"CV Training (Avg)\": [\n",
    "        df_final_cv.iloc[-1][\"Train MAE\"],\n",
    "        df_final_cv.iloc[-1][\"Train RMSE\"],\n",
    "        df_final_cv.iloc[-1][\"Train R2\"]\n",
    "    ],\n",
    "    \"CV Validation (Avg)\": [\n",
    "        df_final_cv.iloc[-1][\"Val MAE\"],\n",
    "        df_final_cv.iloc[-1][\"Val RMSE\"],\n",
    "        df_final_cv.iloc[-1][\"Val R2\"]\n",
    "    ],\n",
    "    \"Test Set (Final)\": [\n",
    "        mean_absolute_error(y_test, y_test_pred),\n",
    "        np.sqrt(mean_squared_error(y_test, y_test_pred)),\n",
    "        r2_score(y_test, y_test_pred)\n",
    "    ]\n",
    "}\n",
    "\n",
    "print(\"\\n>>> 模型性能最终汇总 (HistGradientBoosting + BayesOpt):\")\n",
    "print(pd.DataFrame(final_summary).round(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d8dd4be-32ec-4383-a074-3a7cc497c55c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
